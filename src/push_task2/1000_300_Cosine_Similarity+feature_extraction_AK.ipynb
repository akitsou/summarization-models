{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "718aX95UK0EB"
   },
   "source": [
    "Extractive Summarizer: An extractive summarizer based on a non-deep learning supervised model.\n",
    "1. In particular, you have to train a non-deep learning classification (e.g. logistic regression, SVM) or regression model (e.g. ElasticNet, SVR) that will be used for scoring the sentences of the input document. (0 or 1)\n",
    "2. Then based on the scores you will create a summarizer that attempts to create a summary with the most informative, non-redundant **sentences**. It is up to you which machine learning model and features you will use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lFtfA4cRI1q"
   },
   "source": [
    "## Steps\n",
    "\n",
    "0. Data exploration\n",
    "1. Preprocessing, which will be applied also on the highlights: so the 2 new columns cleaned_articles, cleaned highlights will be use for similarity calculation and feature extraction.\n",
    "2. Feature Extraction (sentence length, sentence position): here also we can try : NER the count of **Named entities** in a sentence , and POS tagging for the count of **VERBS ** (since Verbs can denote events, it can also good practice for event extraction)\n",
    "3. Sentence score calculation: similarity of each sentence of the article with each sentence in the highlights based on their Average word2vec embedding.\n",
    "High scores between pairs denote that the sentence is a good candidate for being added in the summary, so high-scoring sentences will be assigned label 1. Low scoring pairs based on a specific threshold will be assigned label 0.\n",
    "4. Sentence extraction: extract the n first sentences in the document that have been given a value of 1 in the previous step. (i chose the 3 first)\n",
    "5. Summary production: these 3 first sentences should be written in a .txt file , where on the left side, we will have the article id: eg\n",
    "(f001ec5c4704938247d27a44948eebb37ae98d01) and then the 3 extracted sentences as raw text,separated with a fullstop (no lists).\n",
    "6. the content of 'highlights' column should be compared to the content of the 'ml-summary' column and Compute Rouge 2 metrics in a new column which will contain all 3 requested metrics in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zPrqF-gdI8QF",
    "outputId": "3194c5ad-acc0-4f5a-eb58-03213aeee788"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IAhjDoSrgii2",
    "outputId": "89771ef4-f32d-4d5a-efef-b7705c62584b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.16.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting contractions\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\ankar\\anaconda3\\envs\\ai-2\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ankar\\anaconda3\\envs\\ai-2\\lib\\site-packages (from datasets) (1.26.0)\n",
      "Collecting pyarrow>=8.0.0 (from datasets)\n",
      "  Downloading pyarrow-14.0.2-cp311-cp311-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\ankar\\anaconda3\\envs\\ai-2\\lib\\site-packages (from datasets) (2.1.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\ankar\\anaconda3\\envs\\ai-2\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\ankar\\anaconda3\\envs\\ai-2\\lib\\site-packages (from datasets) (4.66.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in c:\\users\\ankar\\anaconda3\\envs\\ai-2\\lib\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.9.2)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.9.1-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets)\n",
      "  Downloading huggingface_hub-0.20.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\ankar\\anaconda3\\envs\\ai-2\\lib\\site-packages (from datasets) (23.1)\n",
      "Collecting pyyaml>=5.1 (from datasets)\n",
      "  Downloading PyYAML-6.0.1-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting textsearch>=0.0.21 (from contractions)\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ankar\\anaconda3\\envs\\ai-2\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.4-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp311-cp311-win_amd64.whl.metadata (32 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ankar\\anaconda3\\envs\\ai-2\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ankar\\anaconda3\\envs\\ai-2\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ankar\\anaconda3\\envs\\ai-2\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ankar\\anaconda3\\envs\\ai-2\\lib\\site-packages (from requests>=2.19.0->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ankar\\anaconda3\\envs\\ai-2\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "     ---------------------------------------- 0.0/289.9 kB ? eta -:--:--\n",
      "     --------- ----------------------------- 71.7/289.9 kB 2.0 MB/s eta 0:00:01\n",
      "     --------------------------------- ---- 256.0/289.9 kB 3.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 289.9/289.9 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
      "  Downloading pyahocorasick-2.0.0-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\ankar\\anaconda3\\envs\\ai-2\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ankar\\anaconda3\\envs\\ai-2\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ankar\\anaconda3\\envs\\ai-2\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\ankar\\anaconda3\\envs\\ai-2\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ankar\\anaconda3\\envs\\ai-2\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.16.0-py3-none-any.whl (507 kB)\n",
      "   ---------------------------------------- 0.0/507.1 kB ? eta -:--:--\n",
      "   --------------------------- ----------- 358.4/507.1 kB 11.2 MB/s eta 0:00:01\n",
      "   --------------------------------------- 507.1/507.1 kB 10.6 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "   ---------------------------------------- 0.0/115.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 115.3/115.3 kB 7.0 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.9.1-cp311-cp311-win_amd64.whl (364 kB)\n",
      "   ---------------------------------------- 0.0/364.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 364.8/364.8 kB 11.4 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.20.1-py3-none-any.whl (330 kB)\n",
      "   ---------------------------------------- 0.0/330.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 330.1/330.1 kB 20.0 MB/s eta 0:00:00\n",
      "Downloading pyarrow-14.0.2-cp311-cp311-win_amd64.whl (24.6 MB)\n",
      "   ---------------------------------------- 0.0/24.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.6/24.6 MB 18.9 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 1.2/24.6 MB 15.1 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 1.8/24.6 MB 13.9 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 2.3/24.6 MB 13.5 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 2.9/24.6 MB 13.4 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 3.5/24.6 MB 13.2 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 4.1/24.6 MB 13.1 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 4.7/24.6 MB 13.1 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 5.3/24.6 MB 13.0 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 5.9/24.6 MB 13.0 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 6.5/24.6 MB 13.0 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 7.1/24.6 MB 13.0 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 7.7/24.6 MB 12.9 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 8.3/24.6 MB 12.9 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 8.5/24.6 MB 12.9 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 8.5/24.6 MB 12.9 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 8.6/24.6 MB 11.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 10.7/24.6 MB 12.6 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 11.2/24.6 MB 13.1 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 11.8/24.6 MB 12.8 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 12.5/24.6 MB 12.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 12.7/24.6 MB 12.8 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 12.7/24.6 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 13.0/24.6 MB 11.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 14.8/24.6 MB 12.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 15.4/24.6 MB 12.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 16.0/24.6 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 16.6/24.6 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 17.2/24.6 MB 12.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 17.7/24.6 MB 12.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 18.4/24.6 MB 12.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 18.4/24.6 MB 12.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 19.5/24.6 MB 14.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 20.0/24.6 MB 13.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 20.6/24.6 MB 12.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.2/24.6 MB 12.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.8/24.6 MB 12.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.4/24.6 MB 12.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.0/24.6 MB 12.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.6/24.6 MB 14.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.6 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.6 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.6/24.6 MB 12.6 MB/s eta 0:00:00\n",
      "Downloading PyYAML-6.0.1-cp311-cp311-win_amd64.whl (144 kB)\n",
      "   ---------------------------------------- 0.0/144.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 144.7/144.7 kB 9.0 MB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
      "   ---------------------------------------- 0.0/135.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 135.4/135.4 kB 7.8 MB/s eta 0:00:00\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp311-cp311-win_amd64.whl (29 kB)\n",
      "Downloading frozenlist-1.4.1-cp311-cp311-win_amd64.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.5/50.5 kB ? eta 0:00:00\n",
      "Downloading yarl-1.9.4-cp311-cp311-win_amd64.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/76.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 76.7/76.7 kB ? eta 0:00:00\n",
      "Installing collected packages: xxhash, pyyaml, pyarrow-hotfix, pyarrow, pyahocorasick, multidict, frozenlist, dill, anyascii, yarl, textsearch, multiprocess, huggingface-hub, aiosignal, contractions, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.9.1 aiosignal-1.3.1 anyascii-0.3.2 contractions-0.1.73 datasets-2.16.0 dill-0.3.7 frozenlist-1.4.1 huggingface-hub-0.20.1 multidict-6.0.4 multiprocess-0.70.15 pyahocorasick-2.0.0 pyarrow-14.0.2 pyarrow-hotfix-0.6 pyyaml-6.0.1 textsearch-0.0.24 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6wDTt9rWYBbL",
    "outputId": "e215b1b0-717c-4eb6-8131-4a0f061b7fe3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ankar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ankar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\ankar\\anaconda3\\envs\\AI-2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ankar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import numpy as np\n",
    "import re\n",
    "import datasets\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('wordnet')\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.models import KeyedVectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "V2lq9gS9raAG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 313M/313M [00:48<00:00, 6.45MB/s] \n",
      "Downloading data: 100%|██████████| 304M/304M [01:10<00:00, 4.33MB/s] \n",
      "Downloading data: 100%|██████████| 155M/155M [00:31<00:00, 4.84MB/s] \n",
      "Downloading data: 100%|██████████| 34.7M/34.7M [00:05<00:00, 6.80MB/s]\n",
      "Downloading data: 100%|██████████| 30.0M/30.0M [00:04<00:00, 6.45MB/s]\n",
      "Generating train split: 100%|██████████| 287113/287113 [00:03<00:00, 87653.73 examples/s] \n",
      "Generating validation split: 100%|██████████| 13368/13368 [00:00<00:00, 123981.09 examples/s]\n",
      "Generating test split: 100%|██████████| 11490/11490 [00:00<00:00, 93165.49 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'validation', 'test'])\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"cnn_dailymail\"\n",
    "dataset_version = \"3.0.0\"\n",
    "\n",
    "dataset = datasets.load_dataset(dataset_name, dataset_version)\n",
    "\n",
    "print(dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the datasets to DataFrames, then to csv to be saved.\n",
    "train_df = pd.DataFrame(dataset['train']) \n",
    "train_df.to_csv('train.csv')\n",
    "val_df = pd.DataFrame(dataset['validation']) \n",
    "val_df.to_csv('val.csv')\n",
    "test_df = pd.DataFrame(dataset['test'])\n",
    "test_df.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ankar\\\\Desktop\\\\summarization-models\\\\src\\\\push_task2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd() # Path of the saved csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLUPuPsXBvCR"
   },
   "outputs": [],
   "source": [
    "df_train =pd.read_csv('/content/train.csv')\n",
    "df_test=pd.read_csv('/content/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kb1CwAoA1bPJ"
   },
   "outputs": [],
   "source": [
    "df_train=df_train.head(1000)\n",
    "df_test=df_test.head(300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e3bj5_42sli"
   },
   "source": [
    "#Preprocessing steps:\n",
    "Remove:\n",
    "- start and end patterns\n",
    "- parenthesis (and everything insinde them)\n",
    "\n",
    "##Also another pattern spotted in the highlights column: NEW: blablaaa , so remove the \"NEW:\" at the beginning of the highlights\n",
    "- expand contractions (won't =will not)\n",
    "- stopwords\n",
    "- 's (sister's =sister)\n",
    "- sentence tokenization\n",
    "- lowercasing\n",
    "- word tokenisation\n",
    "- remove punctuation\n",
    "- non- ascii characters (e.g $ 20 million= 20 million)\n",
    "- lemmatization  \n",
    "\n",
    "**What was not removed and maybe it should be removed is the numbers!!**\n",
    "\n",
    "**Also after stopword removel: Removal of sentences with sentence length <=5 tokens!**\n",
    "We can discuss this\n",
    "The other approach would be to keep the articles which have sentence length 200 (in tokens and highlight length of around 50 tokens, this will leave us with around 30.000 train data!)\n",
    "#Also more cleaning to DO!!!:\n",
    "1. Drop the duplicate rows\n",
    "2. Check & Remove Null data\n",
    "3. Remove https, links, urls,\n",
    "4. remove href:.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2V2E28Y2sFi"
   },
   "outputs": [],
   "source": [
    "def apply_lemmatization(tokens):\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "def apply_stemming(tokens):\n",
    "  porter = PorterStemmer()\n",
    "  return [porter.stem(word) for word in tokens]\n",
    "\n",
    "def clean_text(x):\n",
    "  puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#',\n",
    "              '*', '+', '\\\\', '•', '~', '@', '£',\n",
    "              '·', '_', '{', '}', '©', '^', '®', '`','--', '<', '→', '°', '€', '™', '›', '♥', '←', '×', '§', '″', '′', 'Â',\n",
    "              '█', '½', 'à', '…',\n",
    "              '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―',\n",
    "              '¥', '▓', '—', '‹', '─',\n",
    "              '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸',\n",
    "              '¾', 'Ã', '⋅', '‘', '∞',\n",
    "              '∙', '）', '↓', '、', '│', '（', '»','«', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n",
    "              '¹', '≤', '‡', '√', ]\n",
    "\n",
    "  x = str(x)\n",
    "  for punct in puncts:\n",
    "    x = x.replace(punct, f' {punct} ')\n",
    "  return x\n",
    "\n",
    "def remove_non_ascii(tokens):\n",
    "    return [word for word in tokens if re.match(r'^[^\\x00-\\x7F]+$', word) is None]\n",
    "\n",
    "def remove_urls(text):\n",
    "    # Remove URLs starting with http:// or https://\n",
    "    text = re.sub(r'https?://\\S+', '', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    # Remove HTML tags using BeautifulSoup\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_usernames(text):\n",
    "    # Remove @usernames\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    return text\n",
    "\n",
    "def clean_and_tokenize(article):\n",
    "    # Remove text within parentheses and everything in it\n",
    "    cleaned_article = re.sub(r'\\([^)]*\\)', '', article)\n",
    "\n",
    "    # Split the text using '-- ' as the delimiter\n",
    "    parts = re.split(r'-- ', cleaned_article, maxsplit=1)\n",
    "\n",
    "    # Check if there was a match and reconstruct the text\n",
    "    cleaned_article = parts[1] if len(parts) > 1 else cleaned_article\n",
    "\n",
    "    # Remove 'E-mail to a friend' and anything that follows it\n",
    "    cleaned_article = re.sub(r'E-mail to a friend.*', '', cleaned_article)\n",
    "    #####HERE: add the extra cleaning functions: hmtl, urls, usernames removal)\n",
    "    #cleaned_article=re.sub(r'<a href','',text)\n",
    "    #cleaned_article=re.sub(r'&amp;','',text)\n",
    "    # Remove URLs\n",
    "    #cleaned_article = remove_urls(article)\n",
    "    # Remove HTML tags\n",
    "    #cleaned_article = remove_html_tags(cleaned_article)\n",
    "    # Remove @usernames\n",
    "    #cleaned_article = remove_usernames(cleaned_article)\n",
    "    # Expand word contractions\n",
    "    expanded_article = contractions.fix(cleaned_article)\n",
    "    # Add a period after the closing quotation mark if there is a space and a capital letter\n",
    "    text_with_period = re.sub(r'(\")([ ])([A-Z])', r'\\1.\\2\\3', expanded_article)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text_with_period)\n",
    "    filtered_article = [word for word in word_tokens if word.lower() not in stop_words and word.lower() != \"'s\"]\n",
    "    filtered_article = ' '.join(filtered_article)\n",
    "    #Sentence tokenization\n",
    "    sentences = sent_tokenize(filtered_article)\n",
    "    # Lowercase the text\n",
    "    sentences = [sentence.lower() for sentence in sentences]\n",
    "    # Tokenize sentences\n",
    "    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "    # Remove punctuation using custom clean_text function\n",
    "    tokenized_sentences_no_punct = [\n",
    "        [word for word in word_tokenize(clean_text(sentence)) if word not in string.punctuation]\n",
    "        for sentence in sentences]\n",
    "    tokenized_sentences_no_ascii = [\n",
    "    remove_non_ascii(sentence)\n",
    "    for sentence in tokenized_sentences_no_punct]\n",
    "    # Apply lemmatization\n",
    "    tokenized_sentences_lemmatized = [\n",
    "        apply_lemmatization(sentence)\n",
    "        for sentence in tokenized_sentences_no_ascii\n",
    "        if len(sentence) > 5\n",
    "        ]\n",
    "\n",
    "    # Remove empty lists\n",
    "    tokenized_sentences_lemmatized = [sentence for sentence in tokenized_sentences_lemmatized if sentence]\n",
    "\n",
    "    return tokenized_sentences_lemmatized\n",
    "\n",
    "###################THESE 2 FUNCTIONS HAVE NOT BEEN YET USED IN IMPLEMETATION THEY AREJUST AN IDEA APPROACH###################\n",
    "def get_columns(dataframe, article, highlights):\n",
    "\n",
    "  # Get only the columns we are interested in\n",
    "  dataset = dataframe[[article]]\n",
    "\n",
    "  # Apply the pre-processing function to the dataframe containing the text (feature column)\n",
    "  dataset[article] = dataset[article].apply(clean_and_tokenize)\n",
    "  dataset[highlights] = dataset[highlights].apply(clean_and_tokenize)\n",
    "\n",
    "  print('\\nText done pre-processing!')\n",
    "\n",
    "  X = dataset[article]\n",
    "  Y = dataset[highlights]\n",
    "\n",
    "  return X, Y\n",
    "\n",
    "def data_prepare(df_train, df_test,w2v_model, article, highlights):\n",
    "\n",
    "  # Prepare the training dataset\n",
    "  print('------ Preparing the training dataset... ------')\n",
    "  X,y = get_columns(df_train, article, highlights)\n",
    "\n",
    "  # Prepare the validation/testing dataset\n",
    "  print('\\n------ Preparing the validation/testing dataset... ------')\n",
    "  x1,y1 = get_columns(df_test, article, highlights)\n",
    "\n",
    "\n",
    "  w2vX_train, words_found, matrix_len = find_words_in_w2v(X,w2v_model)\n",
    "  print('Percentage of words found in W2V: ', words_found/matrix_len)\n",
    "\n",
    "  w2vX_test, words_found, matrix_len = find_words_in_w2v(x1,w2v_model)\n",
    "  print('Percentage of words found in W2V: ', words_found/matrix_len)\n",
    "# data_prepare(df_train, df_test,w2v_model='/content/drive/MyDrive/NLU/Task_2/w2v_model/w2v_summ.model','article','highlights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6Wx0VdjVQCA"
   },
   "source": [
    "#Apply cleaning in the articles and highlights of both test and train datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Go5s46L9fgmN"
   },
   "outputs": [],
   "source": [
    "# Apply the function to each row in the 'article' column\n",
    "df_train['cleaned_article'] = df_train['article'].apply(clean_and_tokenize)\n",
    "df_test['cleaned_article'] = df_test['article'].apply(clean_and_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hL5QVq8Mi-zf"
   },
   "outputs": [],
   "source": [
    "df_train['clean_highlights']=df_train['highlights'].apply(clean_and_tokenize)\n",
    "df_test['clean_highlights']=df_test['highlights'].apply(clean_and_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzpJK2JrVYM5"
   },
   "source": [
    "Keep first 50% of tokenized sentences. in the cleaned articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rMoJuT9fd39g"
   },
   "outputs": [],
   "source": [
    "df_train['first_half_sentences'] = df_train['cleaned_article'].apply(lambda sentences: sentences[:len(sentences)//2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUl63K-JZbxz"
   },
   "outputs": [],
   "source": [
    "df_test['first_half_sentences'] = df_test['cleaned_article'].apply(lambda sentences: sentences[:len(sentences)//2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lb03nfatleWK"
   },
   "outputs": [],
   "source": [
    "rows_to_display = [0,1,2]  # Replace with the row indices you want to check\n",
    "\n",
    "for row_index in rows_to_display:\n",
    "    print(f\"Row {row_index}:\")\n",
    "    print(df_train['cleaned_article'].iloc[row_index])\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLZNMPt0ldu-"
   },
   "outputs": [],
   "source": [
    "# Set display options to show all rows and columns\n",
    "#pd.set_option('display.max_rows', None)\n",
    "#pd.set_option('display.max_columns', None)\n",
    "\n",
    "#def compute_max_cosine_similarity(row, threshold=0.3):\n",
    "#    articles = row['cleaned_article']\n",
    "#    highlights = row['clean_highlights']\n",
    "\n",
    "    # Flatten the lists of tokenized sentences into strings\n",
    "#    articles_str = [' '.join(sent) for sent in articles]\n",
    "#    highlights_str = [' '.join(sent) for sent in highlights]\n",
    "\n",
    "    # Combine articles and highlights for fitting CountVectorizer\n",
    "#    all_sentences = articles_str + highlights_str\n",
    "\n",
    "    # Use CountVectorizer to convert sentences to document-term matrices\n",
    "#    vectorizer = CountVectorizer()\n",
    "#    all_matrix = vectorizer.fit_transform(all_sentences).toarray()\n",
    "\n",
    "    # Split the matrices back into articles and highlights parts\n",
    "#    articles_matrix = all_matrix[:len(articles)]\n",
    "#    highlights_matrix = all_matrix[len(articles):]\n",
    "\n",
    "    # Compute cosine similarity for each pair of sentences\n",
    "#    similarity_matrix = cosine_similarity(articles_matrix, highlights_matrix)\n",
    "\n",
    "    # Find the maximum similarity score for each sentence\n",
    "#    max_similarity_scores = similarity_matrix.max(axis=1)\n",
    "#    max_similarity_scores = [round(score, 3) for score in max_similarity_scores]\n",
    "    # Assign labels based on the threshold\n",
    "#    labels = [1 if score >= threshold else 0 for score in max_similarity_scores]\n",
    "#    return labels\n",
    "\n",
    "# Apply the compute_max_cosine_similarity function to each row\n",
    "#df6['max_cosine_similarity'] = df6.apply(compute_max_cosine_similarity, axis=1)\n",
    "\n",
    "#df6['labels'] = df6.apply(compute_max_cosine_similarity, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tbjQa4ce01K"
   },
   "source": [
    "Explode dataframe so that each sentence of each article is represented by a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VNk2qWXlM9Av"
   },
   "outputs": [],
   "source": [
    "# Explode the DataFrame and reset the index\n",
    "df_train_exploded = df_train.explode('first_half_sentences').reset_index(drop=True)\n",
    "df_test_exploded = df_test.explode('first_half_sentences').reset_index(drop=True)\n",
    "\n",
    "# Rename the 'articles' column to 'sentences'\n",
    "df_train_exploded = df_train_exploded.rename(columns={'first_half_sentences': 'sentences'})\n",
    "df_test_exploded = df_test_exploded.rename(columns={'first_half_sentences': 'sentences'})\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(df_train_exploded)\n",
    "print(df_test_exploded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lq4GqvP4m8uo"
   },
   "outputs": [],
   "source": [
    "current_article_id = None\n",
    "article_index = 0\n",
    "\n",
    "def create_article_test_sentence(row):\n",
    "    global current_article_id, article_index\n",
    "\n",
    "    # Check if the current article ID is different from the one in the row\n",
    "    if row['id'] != current_article_id:\n",
    "        current_article_id = row['id']\n",
    "        article_index += 1  # Increment the article index for a new article\n",
    "\n",
    "    return f\"{article_index}-{row.name}\"\n",
    "\n",
    "# Assuming 'id' is the column containing the article IDs\n",
    "df_test_exploded['article-sentence'] = df_test_exploded.apply(create_article_test_sentence, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3enijRjadvuI"
   },
   "source": [
    "Fix the article_sentence  indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lh0fNFFGdyAs"
   },
   "outputs": [],
   "source": [
    "# Extract article and sentence indices\n",
    "df_test_exploded[['article', 'sentence']] = df_test_exploded['article-sentence'].str.split('-', expand=True)\n",
    "\n",
    "# Convert columns to numeric\n",
    "df_test_exploded['article'] = pd.to_numeric(df_test_exploded['article'])\n",
    "df_test_exploded['sentence'] = pd.to_numeric(df_test_exploded['sentence'])\n",
    "\n",
    "# Calculate the correct sentence index for each article\n",
    "df_test_exploded['new_sentence_index'] = df_test_exploded.groupby('article').cumcount()\n",
    "\n",
    "# Create the new article-sentence column\n",
    "df_test_exploded['new_article_sentence'] = df_test_exploded['article'].astype(str) + '-' + df_test_exploded['new_sentence_index'].astype(str)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df_test_exploded = df_test_exploded.drop(['article','article-sentence', 'sentence', 'new_sentence_index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dJZKtrfwegEv"
   },
   "outputs": [],
   "source": [
    "df_test_exploded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHx6OKW9fNc-"
   },
   "source": [
    "Encode sentence position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZnevJiGZRCcK"
   },
   "outputs": [],
   "source": [
    "# Add a new column 'position' based on the sentences' order within each article\n",
    "df_train_exploded['position'] = df_train_exploded.groupby('id').cumcount()\n",
    "df_test_exploded['position'] = df_test_exploded.groupby('id').cumcount()\n",
    "\n",
    "# Calculate the total number of sentences for each article\n",
    "train_article_lengths = df_train_exploded.groupby('id').size()\n",
    "test_article_lengths = df_test_exploded.groupby('id').size()\n",
    "\n",
    "# Gradually reduce the position value for each article\n",
    "df_train_exploded['position'] = 1 - (df_train_exploded['position'] / train_article_lengths[df_train_exploded['id']].values)\n",
    "df_test_exploded['position'] = 1 - (df_test_exploded['position'] / test_article_lengths[df_test_exploded['id']].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knkAxmrNRkY2"
   },
   "outputs": [],
   "source": [
    "# If you want to round the values to a certain decimal place, you can use round()\n",
    "df_train_exploded['position'] = round(df_train_exploded['position'], 2)\n",
    "df_test_exploded['position'] = round(df_test_exploded['position'], 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3H5vp3cU-ur"
   },
   "outputs": [],
   "source": [
    "df_test_exploded.position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_3fkOJJfSPo"
   },
   "source": [
    "W2Vec model! Load the trained model and use it to create average w2v embeddings for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7K13RI9kmBpt"
   },
   "outputs": [],
   "source": [
    "w2v_model=Word2Vec.load('/content/drive/MyDrive/NLU/Task_2/w2v_model/w2v_summ.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fl2BJzg184-5"
   },
   "outputs": [],
   "source": [
    "def find_words_in_w2v(train_dataset, w2vmodel):\n",
    "    # Load the Word2Vec model\n",
    "    if isinstance(w2vmodel, str):\n",
    "        w2v_model = KeyedVectors.load_word2vec_format(w2vmodel, binary=True)\n",
    "    elif isinstance(w2vmodel, Word2Vec):\n",
    "        w2v_model = w2vmodel\n",
    "    else:\n",
    "        raise ValueError(\"Invalid Word2Vec model format\")\n",
    "\n",
    "    # Get the size of the Word2Vec model\n",
    "    embedding_size = w2v_model.vector_size\n",
    "\n",
    "    # Initialize an empty array to store mean embeddings\n",
    "    mean_embeddings = []\n",
    "        # Iterate over each row in the train_dataset\n",
    "    for sentence in train_dataset['sentences']:\n",
    "        # Check if the 'sentences' column is a list\n",
    "        if isinstance(sentence, list):\n",
    "            # Initialize a list to store word embeddings in the current sentence\n",
    "            word_embeddings = []\n",
    "\n",
    "            # Iterate over each word in the sentence\n",
    "            for word in sentence:\n",
    "                # Check if the word is in the Word2Vec model's vocabulary\n",
    "                if word in w2v_model.wv.key_to_index:\n",
    "                    # If the word is found, add its Word2Vec embedding to the list\n",
    "                    word_embeddings.append(w2v_model.wv[word])\n",
    "                else:\n",
    "                    # If the word is not found, initialize it with zero vectors\n",
    "                    word_embeddings.append(np.zeros(embedding_size))\n",
    "\n",
    "            # Calculate the mean of the Word2Vec embeddings for the words in the row\n",
    "            if word_embeddings:\n",
    "                mean_embedding = np.mean(word_embeddings, axis=0)\n",
    "            else:\n",
    "                # If no word embeddings found, use zero vectors\n",
    "                mean_embedding = np.zeros(embedding_size)\n",
    "\n",
    "            # Append the mean embedding to the list\n",
    "            mean_embeddings.append(mean_embedding)\n",
    "        else:\n",
    "            # If 'sentences' is not a list, append a zero vector\n",
    "            mean_embeddings.append(np.zeros(embedding_size))\n",
    "\n",
    "    # Convert the list of mean embeddings into a NumPy array\n",
    "    mean_embeddings_array = np.array(mean_embeddings)\n",
    "\n",
    "    # Create a new column in the dataframe with the mean embeddings\n",
    "    train_dataset['mean_embeddings'] = mean_embeddings_array.tolist()\n",
    "\n",
    "    return train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EbpQMRSahkTa"
   },
   "outputs": [],
   "source": [
    "df_train_exploded= find_words_in_w2v(df_train_exploded, w2v_model)\n",
    "df_test_exploded = find_words_in_w2v(df_test_exploded, w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ba3uWu4kjhxu"
   },
   "source": [
    "#SOS!! The resutls in mean_embeddings are differnt  from those in the 'Cosine_Similarity.ipynb : we have to check and compare the 2 files carefully to find out what is going wrong here!!Because in the 'cosine similarity. ipynb the results are reasonable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CM80lw5EkXr9"
   },
   "source": [
    "Average w2v for each sentence in the list of highlights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1LYUhdE2n8Qd"
   },
   "outputs": [],
   "source": [
    "def calculate_avg_word2vec_list(sentence_list, model):\n",
    "    avg_word2vec_list = []\n",
    "    for sentence in sentence_list:\n",
    "        # Filter out words that are not in the model's vocabulary\n",
    "        valid_words = [word for word in sentence if word in model.wv.key_to_index]\n",
    "        # Check if there are valid words before calculating the average\n",
    "        if valid_words:\n",
    "            # Use numpy's vstack to vertically stack the word vectors\n",
    "            word_vectors = np.vstack([model.wv[word] for word in valid_words])\n",
    "\n",
    "            # Calculate the mean along the first axis (axis=0)\n",
    "            avg_word2vec = np.mean(word_vectors, axis=0)\n",
    "\n",
    "            avg_word2vec_list.append(avg_word2vec)\n",
    "        else:\n",
    "            # If no valid words, append a zero vector\n",
    "            avg_word2vec_list.append(np.zeros(model.vector_size))\n",
    "\n",
    "    return avg_word2vec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89kZM21WoDRF"
   },
   "outputs": [],
   "source": [
    "df_train_exploded['avg_word2vec_highlights'] = df_train_exploded['clean_highlights'].apply(lambda x: calculate_avg_word2vec_list(x, model=w2v_model))\n",
    "df_test_exploded['avg_word2vec_highlights'] = df_test_exploded['clean_highlights'].apply(lambda x: calculate_avg_word2vec_list(x, model=w2v_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSEnKpafozM7"
   },
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(sentence, highlights_avg_word2vec_list):\n",
    "    return [cosine_similarity([sentence], [highlight_sentence])[0][0] for highlight_sentence in highlights_avg_word2vec_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pwEBt-aMpxGA"
   },
   "outputs": [],
   "source": [
    "df_train_exploded['cosine_similarity'] = df_train_exploded.apply(lambda row: calculate_cosine_similarity(row['mean_embeddings'], row['avg_word2vec_highlights']), axis=1)\n",
    "df_test_exploded['cosine_similarity'] = df_test_exploded.apply(lambda row: calculate_cosine_similarity(row['mean_embeddings'], row['avg_word2vec_highlights']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "naiLRohiqi8h"
   },
   "outputs": [],
   "source": [
    "def get_max_cosine_similarity(cosine_similarity_list):\n",
    "    if cosine_similarity_list:\n",
    "        return max(cosine_similarity_list)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PXYx4D-OqkZo"
   },
   "outputs": [],
   "source": [
    "df_train_exploded['max_cosine_similarity'] = df_train_exploded['cosine_similarity'].apply(get_max_cosine_similarity)\n",
    "df_test_exploded['max_cosine_similarity'] = df_test_exploded['cosine_similarity'].apply(get_max_cosine_similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXflo9qsqpfv"
   },
   "outputs": [],
   "source": [
    "df_train_exploded[['sentences', 'clean_highlights', 'max_cosine_similarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "brxmnmzjrD6H"
   },
   "outputs": [],
   "source": [
    "def assign_value(cosine_similarity):\n",
    "    return 1 if cosine_similarity >= 0.85 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZKFUahJrFQ5"
   },
   "outputs": [],
   "source": [
    "df_train_exploded['label'] = df_train_exploded['max_cosine_similarity'].apply(assign_value)\n",
    "df_test_exploded['label'] = df_test_exploded['max_cosine_similarity'].apply(assign_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B07_YaRBrLOQ"
   },
   "outputs": [],
   "source": [
    "df_train_exploded[['sentences', 'clean_highlights', 'max_cosine_similarity', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xpNkIZXys_Y9",
    "outputId": "8f0dc8c3-f3ed-4048-ed1a-15468960705e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               sentences  \\\n",
      "0      [harry, potter, star, daniel, radcliffe, gain,...   \n",
      "1      [daniel, radcliffe, harry, potter, harry, pott...   \n",
      "2      [disappointment, gossip, columnist, around, wo...   \n",
      "3      [plan, one, people, soon, turn, 18, suddenly, ...   \n",
      "4      [thing, like, buying, thing, cost, 10, pound, ...   \n",
      "...                                                  ...   \n",
      "12042  [1915, made, singing, debut, chicago, symphony...   \n",
      "12043  [world, war, gave, recital, benefited, red, cr...   \n",
      "12044  [1918, began, nearly, year, stay, france, sing...   \n",
      "12045  [experience, led, breakdown, loss, singing, vo...   \n",
      "12046  [mental, floss, sing, national, anthem, sporti...   \n",
      "\n",
      "                                        clean_highlights  \\\n",
      "0      [[harry, potter, star, daniel, radcliffe, get,...   \n",
      "1      [[harry, potter, star, daniel, radcliffe, get,...   \n",
      "2      [[harry, potter, star, daniel, radcliffe, get,...   \n",
      "3      [[harry, potter, star, daniel, radcliffe, get,...   \n",
      "4      [[harry, potter, star, daniel, radcliffe, get,...   \n",
      "...                                                  ...   \n",
      "12042  [[president, harding, illegitimate, daughter, ...   \n",
      "12043  [[president, harding, illegitimate, daughter, ...   \n",
      "12044  [[president, harding, illegitimate, daughter, ...   \n",
      "12045  [[president, harding, illegitimate, daughter, ...   \n",
      "12046  [[president, harding, illegitimate, daughter, ...   \n",
      "\n",
      "       max_cosine_similarity  label  sentence_length  \n",
      "0                   0.936883      1               18  \n",
      "1                   0.844604      0                8  \n",
      "2                   0.900218      1               17  \n",
      "3                   0.850446      1               19  \n",
      "4                   0.770758      0               10  \n",
      "...                      ...    ...              ...  \n",
      "12042               0.673646      0               10  \n",
      "12043               0.687963      0               10  \n",
      "12044               0.733622      0                9  \n",
      "12045               0.691515      0                9  \n",
      "12046               0.634928      0                7  \n",
      "\n",
      "[12047 rows x 5 columns]\n",
      "                                              sentences  \\\n",
      "0     [palestinian, authority, officially, became, 1...   \n",
      "1     [formal, accession, marked, ceremony, hague, n...   \n",
      "2     [palestinian, signed, icc, founding, rome, sta...   \n",
      "3     [later, month, icc, opened, preliminary, exami...   \n",
      "4     [member, court, palestinian, may, subject, cou...   \n",
      "...                                                 ...   \n",
      "2618  [come, home, homework, eat, meal, together, ev...   \n",
      "2619  [doyne, first, priority, organization, keep, c...   \n",
      "2620  [order, come, home, need, lost, parent, rare, ...   \n",
      "2621  [usually, involves, going, child, village, mak...   \n",
      "2622  [dig, birth, certificate, death, certificate, ...   \n",
      "\n",
      "                                       clean_highlights  \\\n",
      "0     [[membership, give, icc, jurisdiction, alleged...   \n",
      "1     [[membership, give, icc, jurisdiction, alleged...   \n",
      "2     [[membership, give, icc, jurisdiction, alleged...   \n",
      "3     [[membership, give, icc, jurisdiction, alleged...   \n",
      "4     [[membership, give, icc, jurisdiction, alleged...   \n",
      "...                                                 ...   \n",
      "2618  [[nepal, civil, war, aftermath, inspired, magg...   \n",
      "2619  [[nepal, civil, war, aftermath, inspired, magg...   \n",
      "2620  [[nepal, civil, war, aftermath, inspired, magg...   \n",
      "2621  [[nepal, civil, war, aftermath, inspired, magg...   \n",
      "2622  [[nepal, civil, war, aftermath, inspired, magg...   \n",
      "\n",
      "      max_cosine_similarity  label  sentence_length  \n",
      "0                  0.937048      1               18  \n",
      "1                  0.792088      0                8  \n",
      "2                  0.959672      1               23  \n",
      "3                  0.926584      1               16  \n",
      "4                  0.857863      1                8  \n",
      "...                     ...    ...              ...  \n",
      "2618               0.748436      0               10  \n",
      "2619               0.834599      0                8  \n",
      "2620               0.814787      0               14  \n",
      "2621               0.801795      0               12  \n",
      "2622               0.755319      0               11  \n",
      "\n",
      "[2623 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "def calculate_sentence_length(sentence):\n",
    "    # Convert to string if not already\n",
    "    sentence_str = str(sentence)\n",
    "\n",
    "    # Assuming 'sentence' is a list of tokens\n",
    "    if isinstance(sentence, list):\n",
    "        return len(sentence)\n",
    "\n",
    "    # If 'sentence' is a string, split it into tokens and return the length\n",
    "    elif isinstance(sentence_str, str):\n",
    "        tokens = sentence_str.split()\n",
    "        return len(tokens)\n",
    "\n",
    "    # Handle other cases or return 0 if it's not a recognized format\n",
    "    else:\n",
    "        return 0\n",
    "# Apply the function to the DataFrame\n",
    "df_train_exploded['sentence_length'] = df_train_exploded['sentences'].apply(calculate_sentence_length)\n",
    "df_test_exploded['sentence_length'] = df_test_exploded['sentences'].apply(calculate_sentence_length)\n",
    "\n",
    "# Display the resulting DataFrame with the new 'sentence_length' column\n",
    "print(df_train_exploded[['sentences', 'clean_highlights', 'max_cosine_similarity', 'label', 'sentence_length']])\n",
    "print(df_test_exploded[['sentences', 'clean_highlights', 'max_cosine_similarity', 'label', 'sentence_length']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-dGbqpyuB9Q"
   },
   "outputs": [],
   "source": [
    "\n",
    "df_test_exploded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IAJzIxdnuo8U",
    "outputId": "2035c883-508a-45f8-a500-7957145fb677"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       position                                    mean_embeddings  \\\n",
      "0          1.00  [0.07560794055461884, 0.09687140583992004, 0.0...   \n",
      "1          0.88  [0.33138221502304077, 0.43603840470314026, 0.2...   \n",
      "2          0.75  [0.03037995472550392, 0.05898478999733925, -0....   \n",
      "3          0.62  [0.0943671315908432, 0.020216917619109154, 0.0...   \n",
      "4          0.50  [-0.034966520965099335, 0.18290971219539642, 0...   \n",
      "...         ...                                                ...   \n",
      "12042      0.14  [0.25769147276878357, 0.03035671077668667, 0.0...   \n",
      "12043      0.11  [0.0879194438457489, -0.016580261290073395, 0....   \n",
      "12044      0.08  [0.2691250443458557, -0.052242983132600784, 0....   \n",
      "12045      0.05  [0.25937116146087646, 0.0009072307148016989, 0...   \n",
      "12046      0.03  [0.24269549548625946, 0.4173557460308075, -0.1...   \n",
      "\n",
      "       max_cosine_similarity  label  sentence_length  \n",
      "0                   0.936883      1               18  \n",
      "1                   0.844604      0                8  \n",
      "2                   0.900218      1               17  \n",
      "3                   0.850446      1               19  \n",
      "4                   0.770758      0               10  \n",
      "...                      ...    ...              ...  \n",
      "12042               0.673646      0               10  \n",
      "12043               0.687963      0               10  \n",
      "12044               0.733622      0                9  \n",
      "12045               0.691515      0                9  \n",
      "12046               0.634928      0                7  \n",
      "\n",
      "[12047 rows x 5 columns]\n",
      "     new_article_sentence  position  \\\n",
      "0                     1-0      1.00   \n",
      "1                     1-1      0.92   \n",
      "2                     1-2      0.85   \n",
      "3                     1-3      0.77   \n",
      "4                     1-4      0.69   \n",
      "...                   ...       ...   \n",
      "2618                300-8      0.38   \n",
      "2619                300-9      0.31   \n",
      "2620               300-10      0.23   \n",
      "2621               300-11      0.15   \n",
      "2622               300-12      0.08   \n",
      "\n",
      "                                        mean_embeddings  \\\n",
      "0     [0.13960391283035278, 0.03212155029177666, 0.0...   \n",
      "1     [-0.029112108051776886, 0.19454684853553772, 0...   \n",
      "2     [0.22083032131195068, 0.15697425603866577, 0.2...   \n",
      "3     [0.12400460243225098, 0.16309775412082672, 0.2...   \n",
      "4     [0.26655715703964233, 0.0014545386657118797, 0...   \n",
      "...                                                 ...   \n",
      "2618  [0.17795193195343018, -0.026634614914655685, -...   \n",
      "2619  [0.22666037501767278, 0.020065916469320655, 0....   \n",
      "2620  [0.1462763398885727, 0.03958917781710625, -0.0...   \n",
      "2621  [0.14037583768367767, -0.08733206242322922, -0...   \n",
      "2622  [0.1258070170879364, 0.030715811997652054, 0.0...   \n",
      "\n",
      "      max_cosine_similarity  label  sentence_length  \n",
      "0                  0.937048      1               18  \n",
      "1                  0.792088      0                8  \n",
      "2                  0.959672      1               23  \n",
      "3                  0.926584      1               16  \n",
      "4                  0.857863      1                8  \n",
      "...                     ...    ...              ...  \n",
      "2618               0.748436      0               10  \n",
      "2619               0.834599      0                8  \n",
      "2620               0.814787      0               14  \n",
      "2621               0.801795      0               12  \n",
      "2622               0.755319      0               11  \n",
      "\n",
      "[2623 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "train_columns_to_keep = ['mean_embeddings', 'position','max_cosine_similarity', 'label', 'sentence_length']\n",
    "\n",
    "test_columns_to_keep = ['mean_embeddings', 'position','max_cosine_similarity', 'label', 'sentence_length','new_article_sentence']\n",
    "\n",
    "# Drop columns not in the list\n",
    "df_filtered_train = df_train_exploded.drop(columns=df_train_exploded.columns.difference(train_columns_to_keep), axis=1)\n",
    "df_filtered_test = df_test_exploded.drop(columns=df_test_exploded.columns.difference(test_columns_to_keep), axis=1)\n",
    "\n",
    "# Display the resulting DataFrame with only the desired columns\n",
    "print(df_filtered_train)\n",
    "print(df_filtered_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "400i3h77K552"
   },
   "source": [
    "#Here : to be able to continue with the summary generations from here, we can also keep the columns 'id', sentences and 'higlhights'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96mh_32rvNjV"
   },
   "outputs": [],
   "source": [
    "df_filtered_train.to_csv('sample_trainset1000.csv')\n",
    "df_filtered_test.to_csv('sample_trainset300.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nz1vdx0_hh9g"
   },
   "source": [
    "See the examples from the dataset of Github:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QpZhu88khf_I"
   },
   "outputs": [],
   "source": [
    "#tr=pd.read_csv('/content/drive/MyDrive/NLU/Task_2/Training_Data_Extension_3.csv')\n",
    "#tst=pd.read_csv('/content/drive/MyDrive/NLU/Task_2/Test_Data_Extension_3.csv')\n",
    "#tr\n",
    "#tst"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
