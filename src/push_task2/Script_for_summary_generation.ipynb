{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtmA5_NiJ4gs"
      },
      "source": [
        "We have a file named predictions.csv. This file contains in each row either a 0 or 1 for whether this sentence is to be added in the summary or not.\n",
        "This csv has exaclty the same length with the test.csv file (for 300 rows articles) . So now that we have the values for each sentence we can create a txt file that has on the first column the id of the summary, and in the second column it has the 3 extracted sentences from the column article, which we have preprocessed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pm3txTV3XIRx"
      },
      "outputs": [],
      "source": [
        "!pip install datasets contractions rouge rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zrXNgtBV0ix"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "import datasets\n",
        "from datasets import load_dataset, load_metric\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import string\n",
        "nltk.download('wordnet')\n",
        "import contractions\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('wordnet')\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uD5HrnJWN7U"
      },
      "outputs": [],
      "source": [
        "# Specify the dataset name and version\n",
        "dataset_name = \"cnn_dailymail\"\n",
        "dataset_version = \"3.0.0\"\n",
        "\n",
        "# Load a specific version of the dataset from Hugging Face\n",
        "dataset = datasets.load_dataset(dataset_name, dataset_version)\n",
        "\n",
        "# Convert the dataset to a Pandas DataFrame\n",
        "df = pd.DataFrame(dataset['test'])  # You can use 'train', 'test', or 'validation'\n",
        "df.to_csv('test.csv')\n",
        "#df =pd.read_csv('/content/output_file.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nzte7lMLWVLZ"
      },
      "outputs": [],
      "source": [
        "df1 =pd.read_csv('/content/test.csv')\n",
        "df1=df.head(300)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same preprocessing as for the training"
      ],
      "metadata": {
        "id": "pDeYBPhdOPfl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zg9GKqfpJzuJ"
      },
      "outputs": [],
      "source": [
        "def apply_lemmatization(tokens):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  return [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "def apply_stemming(tokens):\n",
        "  porter = PorterStemmer()\n",
        "  return [porter.stem(word) for word in tokens]\n",
        "\n",
        "def clean_text(x):\n",
        "  puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#',\n",
        "              '*', '+', '\\\\', '•', '~', '@', '£',\n",
        "              '·', '_', '{', '}', '©', '^', '®', '`','--', '<', '→', '°', '€', '™', '›', '♥', '←', '×', '§', '″', '′', 'Â',\n",
        "              '█', '½', 'à', '…',\n",
        "              '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―',\n",
        "              '¥', '▓', '—', '‹', '─',\n",
        "              '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸',\n",
        "              '¾', 'Ã', '⋅', '‘', '∞',\n",
        "              '∙', '）', '↓', '、', '│', '（', '»','«', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n",
        "              '¹', '≤', '‡', '√', ]\n",
        "\n",
        "  x = str(x)\n",
        "  for punct in puncts:\n",
        "    x = x.replace(punct, f' {punct} ')\n",
        "  return x\n",
        "\n",
        "def remove_non_ascii(tokens):\n",
        "    return [word for word in tokens if re.match(r'^[^\\x00-\\x7F]+$', word) is None]\n",
        "\n",
        "def clean_and_tokenize(article):\n",
        "    # Remove text within parentheses and everything in it\n",
        "    cleaned_article = re.sub(r'\\([^)]*\\)', '', article)\n",
        "\n",
        "    # Split the text using '-- ' as the delimiter\n",
        "    parts = re.split(r'-- ', cleaned_article, maxsplit=1)\n",
        "\n",
        "    # Check if there was a match and reconstruct the text\n",
        "    cleaned_article = parts[1] if len(parts) > 1 else cleaned_article\n",
        "\n",
        "    # Remove 'E-mail to a friend' and anything that follows it\n",
        "    cleaned_article = re.sub(r'E-mail to a friend.*', '', cleaned_article)\n",
        "\n",
        "    # Expand word contractions\n",
        "    expanded_article = contractions.fix(cleaned_article)\n",
        "    # Add a period after the closing quotation mark if there is a space and a capital letter\n",
        "    text_with_period = re.sub(r'(\")([ ])([A-Z])', r'\\1.\\2\\3', expanded_article)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(text_with_period)\n",
        "    filtered_article = [word for word in word_tokens if word.lower() not in stop_words and word.lower() != \"'s\"]\n",
        "\n",
        "    # Replace country abbreviations with names\n",
        "    filtered_article = ' '.join(filtered_article)\n",
        "\n",
        "    sentences = sent_tokenize(filtered_article)\n",
        "    # Lowercase the text\n",
        "    sentences = [sentence.lower() for sentence in sentences]\n",
        "    # Tokenize sentences\n",
        "    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
        "    # Remove punctuation using custom clean_text function\n",
        "    tokenized_sentences_no_punct = [\n",
        "        [word for word in word_tokenize(clean_text(sentence)) if word not in string.punctuation]\n",
        "        for sentence in sentences]\n",
        "    tokenized_sentences_no_ascii = [\n",
        "    remove_non_ascii(sentence)\n",
        "    for sentence in tokenized_sentences_no_punct]\n",
        "    # Apply lemmatization\n",
        "    tokenized_sentences_lemmatized = [\n",
        "        apply_lemmatization(sentence)\n",
        "        for sentence in tokenized_sentences_no_ascii\n",
        "        if len(sentence) > 5\n",
        "        ]\n",
        "\n",
        "    # Remove empty lists\n",
        "    tokenized_sentences_lemmatized = [sentence for sentence in tokenized_sentences_lemmatized if sentence]\n",
        "\n",
        "    return tokenized_sentences_lemmatized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9RQ_G3VMiAO"
      },
      "outputs": [],
      "source": [
        "df1['cleaned_article'] = df1['article'].apply(clean_and_tokenize)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YttciPUwXiAr"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Display the DataFrame with the full content of the 'cleaned_article' column\n",
        "print(df1[['cleaned_article']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivu1SLHMbXhZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74d69bcc-ac2c-4c96-99f5-35b8983aa839"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-da0504aae579>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df1['first_half_sentences'] = df1['cleaned_article'].apply(lambda sentences: sentences[:len(sentences)//2])\n"
          ]
        }
      ],
      "source": [
        "df1['first_half_sentences'] = df1['cleaned_article'].apply(lambda sentences: sentences[:len(sentences)//2])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-5VIMtfbhkm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a09f669d-3f87-48af-c573-c1ac6282f64c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                article  \\\n",
            "0     (CNN)The Palestinian Authority officially beca...   \n",
            "1     (CNN)The Palestinian Authority officially beca...   \n",
            "2     (CNN)The Palestinian Authority officially beca...   \n",
            "3     (CNN)The Palestinian Authority officially beca...   \n",
            "4     (CNN)The Palestinian Authority officially beca...   \n",
            "...                                                 ...   \n",
            "2618  Surkhet, Nepal (CNN)Ten years ago, with her hi...   \n",
            "2619  Surkhet, Nepal (CNN)Ten years ago, with her hi...   \n",
            "2620  Surkhet, Nepal (CNN)Ten years ago, with her hi...   \n",
            "2621  Surkhet, Nepal (CNN)Ten years ago, with her hi...   \n",
            "2622  Surkhet, Nepal (CNN)Ten years ago, with her hi...   \n",
            "\n",
            "                                             highlights  \\\n",
            "0     Membership gives the ICC jurisdiction over all...   \n",
            "1     Membership gives the ICC jurisdiction over all...   \n",
            "2     Membership gives the ICC jurisdiction over all...   \n",
            "3     Membership gives the ICC jurisdiction over all...   \n",
            "4     Membership gives the ICC jurisdiction over all...   \n",
            "...                                                 ...   \n",
            "2618  Nepal civil war aftermath inspired Maggie Doyn...   \n",
            "2619  Nepal civil war aftermath inspired Maggie Doyn...   \n",
            "2620  Nepal civil war aftermath inspired Maggie Doyn...   \n",
            "2621  Nepal civil war aftermath inspired Maggie Doyn...   \n",
            "2622  Nepal civil war aftermath inspired Maggie Doyn...   \n",
            "\n",
            "                                            id  \\\n",
            "0     f001ec5c4704938247d27a44948eebb37ae98d01   \n",
            "1     f001ec5c4704938247d27a44948eebb37ae98d01   \n",
            "2     f001ec5c4704938247d27a44948eebb37ae98d01   \n",
            "3     f001ec5c4704938247d27a44948eebb37ae98d01   \n",
            "4     f001ec5c4704938247d27a44948eebb37ae98d01   \n",
            "...                                        ...   \n",
            "2618  275f70b420d88b7c1e6140d5b4586a2ea36a8015   \n",
            "2619  275f70b420d88b7c1e6140d5b4586a2ea36a8015   \n",
            "2620  275f70b420d88b7c1e6140d5b4586a2ea36a8015   \n",
            "2621  275f70b420d88b7c1e6140d5b4586a2ea36a8015   \n",
            "2622  275f70b420d88b7c1e6140d5b4586a2ea36a8015   \n",
            "\n",
            "                                        cleaned_article  \\\n",
            "0     [[palestinian, authority, officially, became, ...   \n",
            "1     [[palestinian, authority, officially, became, ...   \n",
            "2     [[palestinian, authority, officially, became, ...   \n",
            "3     [[palestinian, authority, officially, became, ...   \n",
            "4     [[palestinian, authority, officially, became, ...   \n",
            "...                                                 ...   \n",
            "2618  [[mean, flower, bud, nepali, home, 50, child, ...   \n",
            "2619  [[mean, flower, bud, nepali, home, 50, child, ...   \n",
            "2620  [[mean, flower, bud, nepali, home, 50, child, ...   \n",
            "2621  [[mean, flower, bud, nepali, home, 50, child, ...   \n",
            "2622  [[mean, flower, bud, nepali, home, 50, child, ...   \n",
            "\n",
            "                                              sentences  \n",
            "0     [palestinian, authority, officially, became, 1...  \n",
            "1     [formal, accession, marked, ceremony, hague, n...  \n",
            "2     [palestinian, signed, icc, founding, rome, sta...  \n",
            "3     [later, month, icc, opened, preliminary, exami...  \n",
            "4     [member, court, palestinian, may, subject, cou...  \n",
            "...                                                 ...  \n",
            "2618  [come, home, homework, eat, meal, together, ev...  \n",
            "2619  [doyne, first, priority, organization, keep, c...  \n",
            "2620  [order, come, home, need, lost, parent, rare, ...  \n",
            "2621  [usually, involves, going, child, village, mak...  \n",
            "2622  [dig, birth, certificate, death, certificate, ...  \n",
            "\n",
            "[2623 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "# Explode the DataFrame and reset the index\n",
        "df_test_exploded = df1.explode('first_half_sentences').reset_index(drop=True)\n",
        "\n",
        "# Rename the 'articles' column to 'sentences'\n",
        "df_test_exploded = df_test_exploded.rename(columns={'first_half_sentences': 'sentences'})\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "print(df_test_exploded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1cxcOOFaS0F"
      },
      "outputs": [],
      "source": [
        "df_test_exploded.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add the predictions csv"
      ],
      "metadata": {
        "id": "8TYMVYHxw_Xp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds=pd.read_csv('/content/drive/MyDrive/NLU/Task_2/dataset/predictions.csv')\n",
        "preds"
      ],
      "metadata": {
        "id": "ZTibjv5sw5Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add the dataframe predictionns in the df_test, since they have exactly same shape"
      ],
      "metadata": {
        "id": "rzUKl47YOXm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate the DataFrames along columns\n",
        "combi = pd.concat([df_test_exploded, preds], axis=1)\n",
        "combi"
      ],
      "metadata": {
        "id": "YBdiZc9Vx1B6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract unique 'id' values in the order they appear\n",
        "unique_ids = combi['id'].unique()\n",
        "\n",
        "# Initialize a variable to store all summaries\n",
        "all_summaries = []\n",
        "\n",
        "# Iterate through unique 'id' values in the original order\n",
        "for article_id in unique_ids:\n",
        "    # Filter the DataFrame for the specific 'id'\n",
        "    group = combi[combi['id'] == article_id]\n",
        "    extracted_sentences = group.loc[group['y_pred'] == 1, 'sentences'].tolist()\n",
        "\n",
        "    # Ensure each element in 'extracted_sentences' is a list\n",
        "    extracted_sentences = [sentence if isinstance(sentence, list) else [] for sentence in extracted_sentences]\n",
        "\n",
        "    # Convert numbers to strings and add full stop as a separator\n",
        "    extracted_sentences = [[' '.join(map(str, sentence)) + '.' for sentence in extracted_sentences]]\n",
        "\n",
        "    # Append the summary to the list\n",
        "    summary = f'({article_id}) {\" \".join(\" \".join(sentence) for sentence in extracted_sentences)}'\n",
        "    all_summaries.append(summary)\n",
        "\n",
        "# Create a text file and write all summaries\n",
        "with open('all_summaries.txt', 'w') as file:\n",
        "    file.write('\\n'.join(all_summaries))"
      ],
      "metadata": {
        "id": "I2KTZvCULVi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate Rouge-2 scores\n",
        "def calculate_rouge_scores(highlight, summary, scorer):\n",
        "    scores = scorer.score(highlight, summary)\n",
        "    return {'precision': round(scores['rouge2'].precision, 3),\n",
        "            'recall': round(scores['rouge2'].recall, 3),\n",
        "            'fmeasure': round(scores['rouge2'].fmeasure, 3)}\n",
        "\n",
        "# Extract unique 'id' values in the order they appear\n",
        "unique_ids = combi['id'].unique()\n",
        "\n",
        "# Initialize a variable to store article data and Rouge-2 scores\n",
        "article_data = []\n",
        "\n",
        "# Create a RougeScorer instance\n",
        "scorer = rouge_scorer.RougeScorer(['rouge2'], use_stemmer=False)\n",
        "# Iterate through unique 'id' values in the original order\n",
        "for article_id in unique_ids:\n",
        "    # Filter the DataFrame for the specific 'id'\n",
        "    group = combi[combi['id'] == article_id]\n",
        "    extracted_sentences = group.loc[group['y_pred'] == 1, 'sentences'].tolist()\n",
        "    highlight = group['highlights'].iloc[0]  # Assume the highlight is the same for all sentences\n",
        "\n",
        "    # Convert each element to string and add full stop as a separator\n",
        "    extracted_sentences = [' '.join(map(str, map(str, element))) + '.' if isinstance(element, list) else '' for element in extracted_sentences]\n",
        "\n",
        "    # Calculate Rouge-2 scores\n",
        "    rouge_scores = calculate_rouge_scores(highlight, ' '.join(extracted_sentences), scorer)\n",
        "\n",
        "    # Append the article data to the list\n",
        "    article_data.append({\n",
        "        'id': article_id,\n",
        "        'highlight': highlight,\n",
        "        'summary': ' '.join(extracted_sentences),\n",
        "        'rouge2_scores': rouge_scores,\n",
        "    })\n",
        "\n",
        "# Create a DataFrame from the article data\n",
        "result_df = pd.DataFrame(article_data)\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "print(result_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilGBIJaG2Llu",
        "outputId": "fe7afe5f-e472-4b62-c7b3-77c6394d3f97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           id  \\\n",
            "0    f001ec5c4704938247d27a44948eebb37ae98d01   \n",
            "1    230c522854991d053fe98a718b1defa077a8efef   \n",
            "2    4495ba8f3a340d97a9df1476f8a35502bcce1f69   \n",
            "3    a38e72fed88684ec8d60dd5856282e999dc8c0ca   \n",
            "4    c27cf1b136cc270023de959e7ab24638021bc43f   \n",
            "..                                        ...   \n",
            "295  f626c9207fe036e2ed8e30df692ba94b7999cad7   \n",
            "296  1203b98b4a1739d0f88789ea59f4cceffde40f2d   \n",
            "297  1ea2893555b80472c3b690b00dc9223c8f71e829   \n",
            "298  5c4eab115efd57b8283c0eb0b49b94ba585b8062   \n",
            "299  275f70b420d88b7c1e6140d5b4586a2ea36a8015   \n",
            "\n",
            "                                             highlight  \\\n",
            "0    Membership gives the ICC jurisdiction over all...   \n",
            "1    Theia, a bully breed mix, was apparently hit b...   \n",
            "2    Mohammad Javad Zarif has spent more time with ...   \n",
            "3    17 Americans were exposed to the Ebola virus w...   \n",
            "4    Student is no longer on Duke University campus...   \n",
            "..                                                 ...   \n",
            "295  Saudi officials say 500 Houthi rebels killed, ...   \n",
            "296  20-year-old American dancer makes $240 a month...   \n",
            "297  Six young Minnesotans conspired to sneak into ...   \n",
            "298  CNN's Dr. Sanjay Gupta answers questions about...   \n",
            "299  Nepal civil war aftermath inspired Maggie Doyn...   \n",
            "\n",
            "                                               summary  \\\n",
            "0    palestinian authority officially became 123rd ...   \n",
            "1    according washington state university dog frie...   \n",
            "2    received hero welcome arrived iran sunny frida...   \n",
            "3    five american monitored three week omaha nebra...   \n",
            "4    duke student admitted hanging noose made rope ...   \n",
            "..                                                 ...   \n",
            "295  trip historically made people fleeing africa r...   \n",
            "296  dancer raised california texas left parent eig...   \n",
            "297  group six young minnesota men conspired sneak ...   \n",
            "298                                                      \n",
            "299  mean flower bud nepali home 50 child infant te...   \n",
            "\n",
            "                                         rouge2_scores  \n",
            "0    {'precision': 0.0, 'recall': 0.0, 'fmeasure': ...  \n",
            "1    {'precision': 0.071, 'recall': 0.071, 'fmeasur...  \n",
            "2    {'precision': 0.0, 'recall': 0.0, 'fmeasure': ...  \n",
            "3    {'precision': 0.026, 'recall': 0.024, 'fmeasur...  \n",
            "4    {'precision': 0.03, 'recall': 0.024, 'fmeasure...  \n",
            "..                                                 ...  \n",
            "295  {'precision': 0.0, 'recall': 0.0, 'fmeasure': ...  \n",
            "296  {'precision': 0.043, 'recall': 0.08, 'fmeasure...  \n",
            "297  {'precision': 0.062, 'recall': 0.05, 'fmeasure...  \n",
            "298  {'precision': 0.0, 'recall': 0.0, 'fmeasure': ...  \n",
            "299  {'precision': 0.04, 'recall': 0.026, 'fmeasure...  \n",
            "\n",
            "[300 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Issues: the summary column has no stopwords and pnctuation, whereas the highlights has everything, has no preprocessing.\n",
        "\n",
        "Maybe we should think of smarter way to write the summaries. So tht the original sentences are added in the summary and can compared to the original highlights"
      ],
      "metadata": {
        "id": "avCTnzzgOzTe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Rouge_2 metrics"
      ],
      "metadata": {
        "id": "soYtU9Gd-qYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you already have the 'result_df' DataFrame with 'rouge2_scores' column\n",
        "# Convert 'rouge2_scores' column to a DataFrame with three separate columns\n",
        "rouge2_df = pd.DataFrame(result_df['rouge2_scores'].tolist())\n",
        "\n",
        "# Calculate mean and std for precision, recall, and fmeasure\n",
        "mean_precision = rouge2_df['precision'].mean()\n",
        "std_precision = rouge2_df['precision'].std()\n",
        "\n",
        "mean_recall = rouge2_df['recall'].mean()\n",
        "std_recall = rouge2_df['recall'].std()\n",
        "\n",
        "mean_fmeasure = rouge2_df['fmeasure'].mean()\n",
        "std_fmeasure = rouge2_df['fmeasure'].std()\n",
        "\n",
        "# Display the results\n",
        "print(f\"Mean Precision: {mean_precision:.3f}, Std Precision: {std_precision:.3f}\")\n",
        "print(f\"Mean Recall: {mean_recall:.3f}, Std Recall: {std_recall:.3f}\")\n",
        "print(f\"Mean F-measure: {mean_fmeasure:.3f}, Std F-measure: {std_fmeasure:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbLMIrfTLC_M",
        "outputId": "ddb2aa3b-5f8a-49b8-f454-da0d0b0fb788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Precision: 0.036, Std Precision: 0.045\n",
            "Mean Recall: 0.048, Std Recall: 0.063\n",
            "Mean F-measure: 0.040, Std F-measure: 0.050\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}