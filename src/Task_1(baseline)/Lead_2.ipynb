{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydeT9UuyB3yG",
        "outputId": "b80068b9-7858-4247-9aac-b671e6ac509e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpPTL4XCZnYB"
      },
      "outputs": [],
      "source": [
        "!pip install datasets rouge rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Seb0Pt6NZqex"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from datasets import load_dataset, load_metric\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download(\"punkt\")\n",
        "import numpy as np\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rouge_score import rouge_scorer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BTBe0fDqs8IX"
      },
      "outputs": [],
      "source": [
        "def predict_lead(article,n):\n",
        "  \"\"\"\n",
        "  input a string of multiple sentences,\n",
        "  store the sentences in a list of sentences through sentence tokenisation and\n",
        "  return a string of the n- first sentences of the list of setences\n",
        "  \"\"\"\n",
        "  sentences =sent_tokenize(article,language='english')\n",
        "  return \" \".join(sentences[:n])\n",
        "\n",
        "def generate_lead2_summaries(dataset_name, dataset_version, dataset_split):\n",
        "  \"\"\"\n",
        "  given the dataset name, version and split part,\n",
        "  load it and extract from it the content of the columns 'id','article', 'highlights' is separate variables,\n",
        "  use the strings form the article and extract the 2 first sentences from it and store this content in the predicted_summary variable. Repeat through for-loop for all rows of the dataframe\n",
        "  store the contents of the dataset's 'ids', 'predicted summaries','gold summaries' into lists\n",
        "  and return them\n",
        "  \"\"\"\n",
        "  dataset = load_dataset(dataset_name, dataset_version)\n",
        "  predictions = []\n",
        "  references = []\n",
        "  article_ids = []\n",
        "\n",
        "  for example in dataset[dataset_split]:\n",
        "      article_id = example[\"id\"]\n",
        "      article = example[\"article\"]\n",
        "      gold_summary = example[\"highlights\"]\n",
        "      predicted_summary = predict_lead(article, 2)\n",
        "      predictions.append(predicted_summary)\n",
        "      references.append(gold_summary)\n",
        "      article_ids.append(article_id)\n",
        "  return article_ids,predictions, references\n",
        "\n",
        "def compute_metrics(predictions, references, article_ids, output_txt_path):\n",
        "    \"\"\"\n",
        "    uses the lists of ids, predicted summaries and gold summaries from the previous function,\n",
        "    initialize a list to store information for each summary,\n",
        "    iterate over each pair of reference and hypothesis sentences,\n",
        "    computes the Rouge-2 score, rounds up the rouge scores to 3 decimal points for precision, recall,fmeasure,\n",
        "    creates a dictionary with the summary information\n",
        "    writes the lead2 summaries to a .txt file, which we specify when calling the function\n",
        "    returns the list of dictionaries with the summary information for each article\n",
        "    \"\"\"\n",
        "\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge2'], use_stemmer=True)\n",
        "    summary_info = []\n",
        "\n",
        "    for article_id, prediction, reference in zip(article_ids, predictions, references):\n",
        "        scores = scorer.score(prediction, reference)\n",
        "        rouge2_score = scores['rouge2']\n",
        "        rounded_rouge2_score = {\n",
        "            'precision': round(rouge2_score.precision, 3),\n",
        "            'recall': round(rouge2_score.recall, 3),\n",
        "            'fmeasure': round(rouge2_score.fmeasure, 3)\n",
        "        }\n",
        "\n",
        "        summary_info.append({\n",
        "            'id': article_id,\n",
        "            'highlight': reference,\n",
        "            'lead2-summary': prediction,\n",
        "            'rouge2_score(Lead_2)': rounded_rouge2_score\n",
        "        })\n",
        "\n",
        "    with open(output_txt_path, 'w', encoding='utf-8') as txt_file:\n",
        "        for article_id, prediction in zip(article_ids, predictions):\n",
        "            txt_file.write(f'({article_id}) {prediction}\\n')\n",
        "\n",
        "    return summary_info\n",
        "\n",
        "def create_dataframe(dataset_name, dataset_version, dataset_split, output_txt_path):\n",
        "    \"\"\"\n",
        "    given the dataset's name, version, split part and a specific txt filepath\n",
        "    apply the function to get the first 2 sentences of each article,\n",
        "    compute the rouge2 and write the id and lead2 summary of each article in\n",
        "    the specific txt pilepath\n",
        "    transform the summary info list into a dataframe and return it\n",
        "    \"\"\"\n",
        "    dataset = load_dataset(dataset_name, dataset_version)\n",
        "\n",
        "    article_ids,predictions, references = generate_lead2_summaries(\n",
        "        dataset_name=dataset_name,\n",
        "        dataset_version=dataset_version,\n",
        "        dataset_split=dataset_split\n",
        "    )\n",
        "\n",
        "    summary_info = compute_metrics(predictions, references, article_ids, output_txt_path)\n",
        "\n",
        "    summaries = pd.DataFrame(summary_info)\n",
        "\n",
        "    return summaries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lead2_dataframe = create_dataframe(dataset_name= \"cnn_dailymail\",\n",
        "    dataset_version=\"3.0.0\",\n",
        "    dataset_split='test',\n",
        "    output_txt_path = '/content/drive/MyDrive/NLU/lead2_summs.txt')\n"
      ],
      "metadata": {
        "id": "Msj1oYoO7fTX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lead2_dataframe.to_csv('/content/drive/MyDrive/NLU/lead2_data.csv')"
      ],
      "metadata": {
        "id": "9pbNwc0XO7tI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_mean_std_metrics(dataframe, column_name):\n",
        "  \"\"\"\n",
        "  given the dataframe and the (metric)column name\n",
        "  calculate and return the mean and standard deviation for the 3 metrics\n",
        "  (precision, recall, f-measure) on all rows of the dataframe\n",
        "  \"\"\"\n",
        "  precision_values = []\n",
        "  recall_values = []\n",
        "  fmeasure_values = []\n",
        "\n",
        "  for index, row in dataframe.iterrows():\n",
        "        # Extract precision, recall, and fmeasure from the row\n",
        "      precision = row[column_name]['precision']\n",
        "      recall = row[column_name]['recall']\n",
        "      fmeasure = row[column_name]['fmeasure']\n",
        "      precision_values.append(precision)\n",
        "      recall_values.append(recall)\n",
        "      fmeasure_values.append(fmeasure)\n",
        "\n",
        "  mean_precision = np.mean(precision_values)\n",
        "  mean_recall = np.mean(recall_values)\n",
        "  mean_fmeasure = np.mean(fmeasure_values)\n",
        "\n",
        "  std_precision = np.std(precision_values)\n",
        "  std_recall = np.std(recall_values)\n",
        "  std_fmeasure = np.std(fmeasure_values)\n",
        "\n",
        "  mean_metrics = {\n",
        "        'mean_precision': mean_precision,\n",
        "        'mean_recall': mean_recall,\n",
        "        'mean_fmeasure': mean_fmeasure\n",
        "    }\n",
        "\n",
        "  std_metrics = {\n",
        "        'std_precision': std_precision,\n",
        "        'std_recall': std_recall,\n",
        "        'std_fmeasure': std_fmeasure\n",
        "    }\n",
        "\n",
        "  return mean_metrics, std_metrics\n",
        "\n",
        "mean_metrics, std_metrics = compute_mean_std_metrics(lead2_dataframe, 'rouge2_score(Lead_2)')\n",
        "print(\"Mean Metrics:\")\n",
        "print(f\"Mean Precision: {mean_metrics['mean_precision']:.3f}\")\n",
        "print(f\"Mean Recall: {mean_metrics['mean_recall']:.3f}\")\n",
        "print(f\"Mean F-measure: {mean_metrics['mean_fmeasure']:.3f}\")\n",
        "\n",
        "print(\"\\nStandard Deviation Metrics:\")\n",
        "print(f\"Std Precision: {std_metrics['std_precision']:.3f}\")\n",
        "print(f\"Std Recall: {std_metrics['std_recall']:.3f}\")\n",
        "print(f\"Std F-measure: {std_metrics['std_fmeasure']:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkSEFWorV-2y",
        "outputId": "047f4319-da7a-45c9-eac5-9a6ae3b10baf"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Metrics:\n",
            "Mean Precision: 0.166\n",
            "Mean Recall: 0.159\n",
            "Mean F-measure: 0.156\n",
            "\n",
            "Standard Deviation Metrics:\n",
            "Std Precision: 0.125\n",
            "Std Recall: 0.121\n",
            "Std F-measure: 0.112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ROUGE_2**\n",
        "\n",
        "Given an reference R and a candidate C summmary:\n",
        "ROUGE-2 precision is the ratio of the number of 2-grams in C that appear also in R , over the number of 2-grams in C.\n",
        "ROUGE-2 recall is the ratio of the number of 2-grams in R that appear also in C , over the number of 2-grams in R.\n",
        "\n",
        "Given the mean and standard deviation values in all 3 rouge_2 metrics, we see that the generated summaries have an average 15% ratio of bigrams also found in the gold summary, meaning that the lead2 baseline system  has a consistent low performance. This was an expected behavior, since the lead2 system extracts the first 2 senteces of the artiles, whereas the 'highlights', which are considered the gold summaries in our data capture several points throughout the article.\n",
        "\n",
        "**ROUGE**\n",
        "\n",
        "Pros: it correlates positively with human evaluation, itâ€™s inexpensive to compute and language-independent.\n",
        "Cons: ROUGE does not manage different words that have the same meaning, as it measures syntactical matches rather than semantics."
      ],
      "metadata": {
        "id": "kI6UUHMCnax6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eq5wTxFRqMwW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}